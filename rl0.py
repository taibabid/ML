# -*- coding: utf-8 -*-
"""RL0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jd6DRuGIbbc0NckZZdrjIxmdvY3TH07U

# Importer les librairies
"""

import numpy as np #Numpy pour manipuler notre Dataset en tant que matrice
import matplotlib.pyplot as plt #Matplotlib.pyplot pour visualiser nos donnÃ©es
from sklearn.datasets import make_regression #La fonction make_regression de Sklearn pour gÃ©nÃ©rer un nuage de point (ici on va simuler des donnÃ©es)
from sklearn.linear_model import SGDRegressor #SGDRegressor (qui signifie Stochastic Gradient Descent Regressor) et qui contient le calcul de la Fonction CoÃ»t, des gradients, de lâ€™algorithme de minimisation

"""# CrÃ©er un Dataset"""

np.random.seed(0) #Pour maitriser lâ€™alÃ©atoire, nous allons gÃ©nÃ©rer un tableau de donnÃ©es (ğ’™, ğ’š) alÃ©atoires
x, y = make_regression(n_samples=100, n_features=1, noise=10) #La fonction prend comme arguments le nombre dâ€™Ã©chantillons Ã  gÃ©nÃ©rer, le nombre de variables et le bruit puis nous retourne deux vecteurs ğ’™ et ğ’š.
plt.scatter(x, y) #pour visualiser nos donnÃ©es

"""# DÃ©velopper le modÃ¨le et lâ€™entraÃ®ner"""

model = SGDRegressor(max_iter=100, eta0=0.0001) #On dÃ©fini nos model depuis le gÃ©nÃ©rateur SGDRegressor en entrant le nombre dâ€™itÃ©rations que le Gradient Descent doit effectuer ainsi que le Learning Rate
model.fit(x,y) #On utilise la fonction fit pour lâ€™entraÃ®ner

print('Coeff R2 =', model.score(x, y)) #On utilise la fonction score qui calcule le coefficient de dÃ©termination entre le modÃ¨le et les valeurs ğ’š de notre Dataset pour observer la prÃ©cision de notre modÃ¨le
plt.scatter(x, y) #pour visualiser nos donnÃ©es
plt.plot(x, model.predict(x), c='red', lw = 3) #On utilise notre modÃ¨le pour faire de nouvelles prÃ©dictions avec la fonction predict et tracer ces rÃ©sultats avec la fonction plt.plot

"""Notre modÃ¨le semble vraiment mauvais. Câ€™est parce que nous ne lâ€™avons pas entraÃ®nÃ© suffisamment longtemps et parce que le Learning rate Ã©tait trop faible. Aucun problÃ¨me, il est possible de le rÃ©-entraÃ®ner avec de meilleurs hyper-paramÃ¨tres.

En Machine Learning, les valeurs qui fonctionnent bien pour la plupart 
des entraÃ®nements sont :

â€¢ Nombre dâ€™itÃ©rations = 1000

â€¢ Learning rate = 0.001
"""

model = SGDRegressor(max_iter=1000, eta0=0.001) 
model.fit(x,y)

print('Coeff R2 =', model.score(x, y)) 
plt.scatter(x, y) 
plt.plot(x, model.predict(x), c='red', lw = 3)

"""Notre modÃ¨le de Machine Learning fonctionne vraiment bien avec un coefficient ğ‘…2 = 94%. Maintenant on peut en servir pour faire de bonnes prÃ©dictions ! """